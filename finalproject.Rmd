---
title: "STA232A - Final Project"
author: "Cassie Xu"
date: "12/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
#install.packages("glmnet")
packages = c("mapview","readxl","ggplot2","GGally","ggmap","psych","naniar",
             "dplyr","DataCombine", "egg", "car", "MASS", "class","naivebayes",
             "glmnet", "caret", "stringr")
lapply(packages, require, character.only = TRUE)

real = read_excel("/Users/xuchenghuiyun/Desktop/PhD/2020 Fall/232 A/STA232FinalProject/Real_estate_valuation.xlsx")[-1]
colnames(real) = c("date","age","MRT","store", "latitude","longitude","price")
```

#### Interacitive & Informatic Maps

```{r}
# interactive plot, but does not show information of price
real_estates <- sf::st_as_sf(real, coords = c("longitude", "latitude"), crs = 4326)
m = mapview(real_estates)
```

```{r}
#real$price_cut = ifelse(real$price>=38,">=38","<38")

map = get_stamenmap(bbox = c(left = 121.4734, bottom = 24.93206, right =
121.5664, top = 25.0146), zoom = 15, maptype = "terrain")

ggmap(map) +
  geom_point(data = real, aes(x = longitude, y = latitude, color=price),
            alpha = 1) +
    labs(x="longitude", y="latitude", title="") +
  scale_colour_gradient(
  low = "pink",
  high = "black")

#ggmap(map) + 
#  geom_density_2d(data = real, aes(x = longitude, y = latitude))
```


#### Exploratory Analysis 

##### Missing Data Check

```{r}
vis_miss(real)
```

No missing data present.

##### Descriptive Statistics

```{r}
des = as.data.frame(describe(real))[-c(6,7)]
knitr::kable(des, align = "c")
```

We do not have super off data points.

##### Correlation Plots & Multicollinearity Check

```{r}
ggcorr(real, label = T)
ggpairs(real, lower = list(continuous = wrap("smooth_loess", alpha = 1, size = 1, color='#00BFC4')), upper = list(continuous = wrap("cor", alpha = 1, size = 3))) +
  theme(axis.text.x = element_text(size = 6, angle = 90),
        axis.text.y = element_text(size = 6))

# condition number calculation
x = scale(real)
lambda = eigen(t(x)%*%x)
condition = lambda$values[1] / lambda$values[7]


ggplot(real, aes(x = longitude, y = price)) + 
  geom_point() + 
  theme_bw() +
  geom_smooth()
```

From the correlation plots above, it seems that house age and transaction date do not have much correlation with price. *Since they all somewhat are related to geographic location, correlations between factors "distance to nearest MRT", "number of convenient stores", "longitude" and "latitude" are not negligible and should be taken care of*.

Since the condition number is `r round(condition,3)` which is smaller than 100, we can conclude that there does not exist collinearity among the predictors.



#### Regression Analysis

##### Model Selection: AIC, BIC, 10-fold CV Subset Regression

```{r}
n = dim(real)[1]
p = dim(real)[2]

tmp = paste0(combn(x=colnames(real[,-7]),m=2)[1,],"*",combn(x=colnames(real[,-7]),m=2)[2,])
formula = as.formula(paste("price", "~", paste(tmp, collapse=" + "), "+I(age^2)+I(latitude^2)+I(longitude^2)"))

fit.int = lm(formula, data = real)
#summary(fit.int)
lmint.both.BIC = step(fit.int, k=log(n), direction="both", test="F", trace=T)
lm.both.AIC = step(fit.int, k=2, direction="both", test="F", trace=T)

sub = leaps::regsubsets(formula, data=real, method="exhaustive")
reg.sum = summary(sub)
coefs = coef(sub, 8)
nams = names(coefs)
paste(nams, collapse=" + ")
#par(mfrow=c(2,2))
plot(sub, scale="bic") 
plot(sub, scale="Cp") 
plot(sub, scale="adjr2") 
plot(sub, scale="r2")
#Since the top two models both have adjusted R2 = 0.58, they are equally good in terms of adjusted R2

# par(mfrow=c(2,2))
# plot(x=1:8,reg.sum$cp,xlab="number of predictors",ylab="Cp",type="b");points(which.min(reg.sum$cp),reg.sum$cp[which.min(reg.sum$cp)],col ="red",cex=2,pch=20)
# 
# plot(x=1:8,reg.sum$bic,xlab="number of predictors",ylab="BIC",type="b");points(which.min(reg.sum$bic),reg.sum$bic[which.min(reg.sum$bic)],col ="red",cex=2,pch=20)
# 
# plot(x=1:8,reg.sum$adjr2,xlab="number of predictors",ylab="Adjusted R2",type="b");points(which.max(reg.sum$adjr2),reg.sum$adjr2[which.max(reg.sum$adjr2)],col ="red",cex=2,pch=20)
# 
# plot(x=1:8,reg.sum$rsq,xlab="number of predictors",ylab="R2",type="b");points(which.max(reg.sum$rsq),reg.sum$rsq[which.max(reg.sum$rsq)],col ="red",cex=2,pch=20)
```

```{r eval=F}
# Cross validation regsubset
set.seed(1)
train=sample(c(TRUE,FALSE),nrow(real),rep=TRUE)
test=(!train)

predict.regsubsets = function(object, newdata, id, ...) {
    form = as.formula(object$call[[2]])
    mat = model.matrix(form, newdata)
    coefi = coef(object, id = id)
    mat[, names(coefi)] %*% coefi
}

k=10
set.seed (1)
folds=sample(1:k,nrow(real),replace =TRUE)
cv.errors=matrix(NA,k,12,dimnames=list(NULL,paste(1:12)))
for(j in 1:k) {
  best.fit=leaps::regsubsets(price ~ date * age + date * MRT + date * store + date * latitude + date * longitude + age * MRT + age * store + age * latitude + 
    age * longitude + MRT * store + MRT * latitude + MRT * longitude + 
    store * latitude + store * longitude + latitude * longitude, data=real[folds!=j,], nvmax = 12)
  for(i in 1:12) {
    pred = predict(best.fit, real[folds==j,], id=i)
    cv.errors[j,i]=mean((real$price[folds==j]-pred)^2)
  }
}

mean.cv.errors=apply(cv.errors,2,mean)
plot(mean.cv.errors,type="b");points(which.min(mean.cv.errors),mean.cv.errors[which.min(mean.cv.errors)],col="red",pch=20, cex=2)

reg.best = leaps::regsubsets(price ~ date * age + date * MRT + date * store + date * latitude +  date * longitude + age * MRT + age * store + age * latitude + 
    age * longitude + MRT * store + MRT * latitude + MRT * longitude + 
    store * latitude + store * longitude + latitude * longitude,data=real, nvmax = 8)
coef(reg.best, 8)
```

##### Log-transformed Response - Model Fitting

```{r}
formula = as.formula(paste("log(price)", "~", paste(tmp, collapse=" + "), "+I(age^2)+I(latitude^2)+I(longitude^2)"))

fit.int = lm(formula, data = real)
lmint.both.BIC = step(fit.int, k=log(n), direction="both", test="F", trace=T)
lm.both.AIC = step(fit.int, k=2, direction="both", test="F", trace=T)

sub = leaps::regsubsets(formula, data=real, method="exhaustive")
reg.sum = summary(sub)
plot(sub, scale="bic") 
# par(mfrow=c(2,2))
# plot(x=1:8,reg.sum$cp,xlab="number of predictors",ylab="Cp",type="b");points(which.min(reg.sum$cp),reg.sum$cp[which.min(reg.sum$cp)],col ="red",cex=2,pch=20)
# 
# plot(x=1:8,reg.sum$bic,xlab="number of predictors",ylab="BIC",type="b");points(which.min(reg.sum$bic),reg.sum$bic[which.min(reg.sum$bic)],col ="red",cex=2,pch=20)
# 
# plot(x=1:8,reg.sum$adjr2,xlab="number of predictors",ylab="Adjusted R2",type="b");points(which.max(reg.sum$adjr2),reg.sum$adjr2[which.max(reg.sum$adjr2)],col ="red",cex=2,pch=20)
# 
# plot(x=1:8,reg.sum$rsq,xlab="number of predictors",ylab="R2",type="b");points(which.max(reg.sum$rsq),reg.sum$rsq[which.max(reg.sum$rsq)],col ="red",cex=2,pch=20)
```

#### LOOCV
```{r}
train.control = trainControl(method="LOOCV")
###AIC method
model1 = train(price ~ date + age + MRT + store + latitude + longitude + 
    I(age^2) + age:longitude + MRT:store + MRT:latitude + store:latitude, data = real, method = "lm", trControl = train.control)
###BIC method
model2 = train(price ~ date + age + MRT + store + latitude + I(age^2) + 
    MRT:store + MRT:latitude + store:latitude, data = real, method = "lm", trControl = train.control)
###regsubsets method
model3 = train(price ~ age + date + MRT + store +latitude+ I(age^2) + date:age + date:latitude + MRT:store + MRT:latitude + store:latitude, data = real, method = "lm", trControl = train.control)
###full model
model4 = train(formula, data = real, method = "lm", trControl = train.control)

RMSE = c(model1$results$RMSE, model2$results$RMSE, model3$results$RMSE, model4$results$RMSE)
R2 = c(model1$results$Rsquared, model2$results$Rsquared, model3$results$Rsquared,
       model4$results$Rsquared)
MAE = c(model1$results$MAE, model2$results$MAE, model3$results$MAE,
       model4$results$MAE)
table.LOOCV = data.frame(RMSE, R2, MAE)
rownames(table.LOOCV) = c('M1', 'M2', 'M3', 'M4')
knitr::kable(table.LOOCV)

### Final model diagnostics
final.fit = lm(log(price) ~ date + age + MRT + store + latitude + 
    MRT:store + MRT:latitude + store:latitude,
               data = real)
round(summary(final.fit)$coefficients,7)[,1:3] %>% as.data.frame()
par(mfrow = c(2,2))
plot(final.fit, which = 1:4)

vif(final.fit)
```

##### Model Diagnostics

```{r}
plot(lmint.both.BIC)

# shapiro.test(lmint.both.BIC$residuals) # H0: Errors are normally distributed: Since p-value is less than 0.05, it is significant to reject the null hypothesis that the errors are normally distributed. Hence the assumption of normality is violated.
# durbinWatsonTest(lmint.both.BIC) # H0:Errors are uncorrelated: Since p-value is less than 0.05, reject H0, which means errors are correlated, hence the uncorrelation assumption is violated.
# ncvTest(lmint.both.BIC) # H0:Errors have a constant vairance: Since p-value is less than 0.05, it is significant to reject the H0 and which means the constant variance assumption is violated.
```























#### Classification

##### Response Variable Manipulation

```{r}
ggplot(real, aes(x=price)) + 
  geom_histogram(binwidth = 5) + 
  xlab("House Price") + 
  ylab("Count") + 
  theme_bw() + geom_vline(xintercept = 37.78, 
                color = "red", size=1.5)

ggplot(real, aes(x=log(price))) + 
  geom_histogram() + 
  xlab("Log-transformed House Price") + 
  ylab("Count") + 
  theme_bw() + geom_vline(xintercept = log(37.78), 
                color = "red", size=1.5)

price_c = cut(real$price, breaks = c(7.49,37.78,118),labels = c("low","high"))
real$price_c = price_c
# table(price_c)
real$price_c = ifelse(real$price_c=="low","0","1")
#mean(real$price[-271])
```

From the histogram, it seems that the response variable "price" is right skewed. For houses' price smaller than 90 * 10000 New Taiwan Dollar/Ping, the distribution is pretty normal, and the only "outlier" is the one that is 117.5 * 10000 New Taiwan Dollar/Ping. Thus, we can divide the variable into four levels, which are "low", "median-low", "median-high" and "high". 

```{r}
# build train and test data
# split the data into train and test
smp_size <- floor(0.75 * nrow(real))

## set the seed to make the partition reproducible
set.seed(12345)
train_ind <- sample(seq_len(nrow(real)), size = smp_size)

train <- real[train_ind, ]
test <- real[-train_ind, ]

# ##Separate out the outcome variable:
# class <- as.data.frame(real$price_c)
# test_class <- class[-train_ind, ]
# train_class <- class[train_ind, ]

##Scale the data
standardized_real <- cbind(scale(real[,-which(colnames(real)=="price_c")]),real[which(colnames(real)=="price_c")])
## split into predictors and outcome train and test data
test_norm <- standardized_real[-train_ind, ]
train_norm <- standardized_real[train_ind, ]
```

```{r eval=F}
##### Logistic Regression
logistic.error <- function(Train, Test) {
  logistic.out <- glm(price_c ~ ., data=Train, family="binomial") 
  pred.class <- predict(logistic.out, Test, type="response")
  pred.class <- ifelse(pred.class > 0.5, 1, 0) 
  mean(Test$price_c != pred.class)
}

##Get misclassification error rate in validation data
pred.error.lr <- round(logistic.error(train, test), digits=3)
```

##### Elastic Net

```{r}
train_norm$price_c <- as.factor(as.character(train_norm$price_c))
test_norm$price_c <- as.factor(as.character(test_norm$price_c))

x_train <- model.matrix(price_c~.-price, train_norm)[,-1]
y_train <- train_norm$price_c
x_test <- model.matrix(price_c~.-price, test_norm)[,-1]
y_test <- test_norm$price_c

lambda.grid <- 10^seq(9,-3,length = 100)
alpha.grid <- seq(0, 1, by = 0.1)

set.seed(12345)
foldid = sample(1:10,size=nrow(x_train),replace=TRUE)

pred.ela <- function(alpha) {
  cv.lambda <- cv.glmnet(x_train, y_train,family='binomial',
                         alpha=alpha, lambda = lambda.grid, foldid=foldid)
  
  fit.ela <- glmnet(x_train, y_train,family='binomial',
                           alpha=alpha, lambda=cv.lambda$lambda.1se)
  pred <- predict(fit.ela, x_test)
  pred <- ifelse(pred > 0.5, "1", "0")
  error <- mean(pred != data.matrix(y_test))
  error
}

error.ela <- sapply(alpha.grid, pred.ela)
error.ela.a <- cbind(as.data.frame(error.ela), as.data.frame(alpha.grid))
colnames(error.ela.a) <- c("pred error", "alpha")
pred.error.ela <- error.ela.a[which.min(error.ela),]
```

```{r}
##### ---------------------------------- KNN
##Create function for misclassification rate from KNN; use default k=1
KNN.error <- function(Train, Test, k=1) {
  Train.KNN <- Train[, which(names(Train) !="price_c")]
  Test.KNN <- Test[,which(names(Test) !="price_c")]
  Test.class <- Test$price_c
  Train.class <- Train$price_c
  
  set.seed(1)
  pred.class=knn(Train.KNN, Test.KNN, Train.class, k=k)
  mean(Test.class != pred.class)
}


##### ---------------------------------- KNN CV
##Create function for misclassification rate from KNN after finding best k through CV
KNN.CV.error <- function(Train, Test, k, m) {
  M <- m # define M as the number of folds
  n <- nrow(Train) # define number of obs in training data
  k_values=1:k
  num_k=length(k_values)
  
  ##create data frame to store CV errors
  cv_error_df <- matrix(0, nrow=num_k, ncol=M) %>%
    as.data.frame(.) %>%
    mutate(k=k_values)
    colnames(cv_error_df) <- str_replace(colnames(cv_error_df), 
                                         'V', 'fold')
  set.seed(3124)
  for(m in 1:M) {
    CVdata <- Train
    CVdata <- CVdata[sample(nrow(CVdata)),]
    folds <- cut(seq(1,nrow(CVdata)),breaks=5,labels=FALSE)
    testIndexes <- which(folds == m, arr.ind=TRUE)
    cv_tr <- CVdata[-testIndexes, ]
    cv_tst <- CVdata[testIndexes, ]
    
    for(i in 1:num_k){
      K <- k_values[i]
      errs <- KNN.error(cv_tr, cv_tst, K)
      cv_error_df[i, paste0('fold',m)] <- errs
    }
  }
  
  cv_error_df <- as.data.frame(cv_error_df)
  
  # compute the mean cv error for each value of k
  cv_mean_error <- cv_error_df[,which(names(cv_error_df) !="k")]
  cv_mean_error <- rowMeans(cv_mean_error)
  cv_mean_error
  
  final_k <- which(cv_mean_error==min(cv_mean_error))[1]
  KNN.error(Train, Test, final_k)
}

##### ---------------------------------- LDA
##Create function for misclassification rate from LDA
LDA.error <- function(Train, Test) {
  lda.out <- lda(price_c ~ ., data=Train)
  pred.class = predict(lda.out, Test)$class
  mean(Test$price_c != pred.class)
}


##### ---------------------------------- QDA
##Create function for misclassification rate from QDA
QDA.error <- function(Train, Test) {
  qda.out <- qda(price_c ~ ., data=Train)
  pred.class = predict(qda.out, Test)$class
  mean(Test$price_c != pred.class)
}
```

```{r}
lambda.grid <- 10^seq(9,-3,length = 100)
set.seed(12345)
foldid = sample(1:10,size=nrow(x_train),replace=TRUE)

pred.ela.fix <- function(x_Train, y_Train, x_Test, y_Test) {
  cv.lambda <- cv.glmnet(x_Train, y_Train,family='binomial',
                         alpha=0.9, lambda = lambda.grid, foldid=foldid)
  
  fit.ela <- glmnet(x_Train, y_Train,family='binomial',
                           alpha=0.9, lambda=cv.lambda$lambda.1se)
  pred <- predict(fit.ela, x_Test)
  pred <- ifelse(pred > 0.5, "1", "0")
  error <- mean(pred != data.matrix(y_Test))
  error
}


##Get errors
KNN.errors = NULL
KNN.CV.errors = NULL
LDA.errors = NULL
QDA.errors = NULL
ELA.errors = NULL

smp_size = floor(0.75 * nrow(standardized_real))

for (i in 1:100) {
  train_ind = sample(seq_len(nrow(standardized_real)), size = smp_size)
  Train = standardized_real[train_ind, ]
  Test = standardized_real[-train_ind, ]

  x_train = model.matrix(price_c~.-price, Train)[,-1]
  y_train = Train$price_c
  x_test = model.matrix(price_c~.-price, Test)[,-1]
  y_test = Test$price_c

  #Train = train
  #Test = test

  ELA.errors[i] = pred.ela.fix(x_train,y_train,x_test,y_test)
  KNN.errors[i] = KNN.error(Train, Test)
  KNN.CV.errors[i] = KNN.CV.error(Train, Test, k=8, m=5)
  LDA.errors[i] = LDA.error(Train, Test)
  QDA.errors[i] = QDA.error(Train, Test)
}

# make data frame of errors
Errors = data.frame(Error.rate=c(ELA.errors, KNN.errors, KNN.CV.errors, LDA.errors, QDA.errors),  Method=c(rep("Elastic Net", 100), rep("KNN-1", 100), rep("KNN CV", 100), rep("LDA", 100), rep("QDA", 100)))
Errors$Method = factor(Errors$Method, levels=c("Elastic Net", "KNN-1", "KNN CV", "LDA", "QDA"))

# plot errors
boxplot(Error.rate ~ Method, data=Errors, col=c("#F8766D", "cornflowerblue", "#7CAE00", "#C77CFF", "pink"), ylab="Error Rate", main="", cex.axis=1)

Errors %>%
  group_by(Method) %>%
  summarise(mean = mean(Error.rate), sd = sd(Error.rate),
            min = min(Error.rate), max = max(Error.rate)) %>%
  mutate_if(is.numeric, round, 3) %>%
  as.data.frame()
```

```{r}
colorfun = function (var) {
  ggplot(real, aes(x = var)) +
  geom_histogram(aes(color = price_c, fill = price_c), 
                position = "identity", bins = 30, alpha = 0.4) +
  scale_color_manual(values = c("#00AFBB", "#E7B800")) +
  scale_fill_manual(values = c("#00AFBB", "#E7B800")) + 
  labs(color = "Price",fill = "Price")
}

colorfun(real$date)
colorfun(real$age)
colorfun(real$MRT)
colorfun(real$store)
colorfun(real$latitude)
colorfun(real$longitude)
```




#### Generalized Linear Model - Baseline Odds

```{r}
###proportional odds model
library(MASS)
real_c = data.frame(real[-1], price_c)
names(real_c)[8] = "price_c"
fit.plr = polr(price_c ~ .-price,data = real_c)
summary(fit.plr)
prd_prob.po = predict(fit.plr, real_c, type="prob")
prd_labl.po = predict(fit.plr, real_c)
label = data.frame(price_c,prd_labl_po)
sum(label[,1]!=label[,2])
###baseline odds model
library(nnet)
fit.bo = multinom(price_c~.-price,data=real_c)
summary(fit.bo)
prd_prob.bo = predict(fit.bo, type = 'prob')
prd_labl.bo = predict(fit.bo)
#label2 = data.frame(price_c,prd_labl_bo)
#sum(label2[,1]!=label2[s,2])
t.est1 = summary(fit.bo)$coefficients[1,]/summary(fit.bo)$standard.errors[1,]
table.est1 = data.frame(Estimate1=summary(fit.bo)$coefficients[1,],
                        std.error1=summary(fit.bo)$standard.errors[1,],
                        t_value1=round(t_est1,3))
t.est2 = summary(fit.bo)$coefficients[2,]/summary(fit.bo)$standard.errors[2,]
table.est2 = data.frame(Estimate2=summary(fit.bo)$coefficients[2,],
                        std.error2=summary(fit.bo)$standard.errors[2,],
                        t_value2=round(t_est2,3))
table.est = data.frame(table.est1, table.est2)
knitr::kable(table.est)
obslabel <- matrix(0, nrow = nrow(real), ncol = length(levels(price_c)))
for (i in seq_len(nrow(real))) obslabel[i,which(colnames(prd_prob.bo)==real_c$price_c[i])] = 1
resP.bo<-(obslabel-prd_prob.bo)/sqrt(prd_prob.bo*(1-prd_prob.bo))
par(mfrow=c(1,2))
plot(fit.bo$fitted.values[,2],resP.bo[,2],pch=16, cex=0.6, 
     ylab='Pearson Residuals', xlab='Fitted Values',
#     ylim=c(-10,10),
     main='Baseline Odds Model - Medium Level')
lines(smooth.spline(fit.bo$fitted.values[,2],resP.bo[,2], spar=2), col=2)
lines(ksmooth(fit.bo$fitted.values[,2],resP.bo[,2], "normal", bandwidth=0.1), col='green')
abline(h=0, lty=2, col='grey')
plot(fit.bo$fitted.values[,3],resP.bo[,3],pch=16, cex=0.6, 
     ylab='Pearson Residuals', xlab='Fitted Values',
#     ylim=c(-10,10),
     main='Baseline Odds Model - High Level')
lines(smooth.spline(fit.bo$fitted.values[,3],resP.bo[,3], spar=2), col=2)
lines(ksmooth(fit.bo$fitted.values[,3],resP.bo[,3], "normal", bandwidth=0.1), col='green')
abline(h=0, lty=2, col='grey')
library(lawstat)
runs.test(y = resP.bo, plot.it = FALSE)
```

```{r eval=F}
##### Bagging and Random Forest
require(randomForest)
#Bagged tree
rf.obj = randomForest(price_c ~.-price_c,
                    data=real, subset = train_ind,
                    mtry=13, ntree=500, importance=TRUE)
pred = predict(rf.obj, newdata=test[-which(colnames(test)=="price_c")])
cm = table(as.matrix(test[,which(colnames(test)=="price_c")]), pred)
bt.test.error = 1-sum(diag(cm)/sum(cm)) # 0.4903846

#Random Forest
rf.obj=randomForest(price_c ~.-price_c,
                    data=real,subset=train_ind,mtry=4,ntree=500,importance=TRUE)
pred = predict(rf.obj, newdata=test[-which(colnames(test)=="price_c")])
cm = table(as.matrix(test[,which(colnames(test)=="price_c")]), pred)
rf.test.error = 1-sum(diag(cm)/sum(cm)) # 0.7692308
# Two measures of variable importance are reported. The former is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is excluded from the model. The latter is a measure total decrease in node impurity that results from splits over that variable, averaged over all trees (Gini indec). In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance.
```

```{r eval=F}
##### GBM and Adaboost
require(gbm)
## gbm
gbm_lambda <- function(depth) {
  lambda.grid <- c(0.01,0.001)
  n.lambda <- 1:length(lambda.grid)
  err.grid <- rep(NA, length=length(n.lambda))
  B.grid <- rep(NA, length=length(n.lambda))

  for (i in n.lambda) {
    set.seed(13456)
    train$price_c = as.numeric(as.character(train$price_c))
    test$price_c = as.numeric(as.character(test$price_c))
    gbm_1 <- gbm(price_c~.-price_c, data = train, distribution = "bernoulli",
                 n.trees=2000, shrinkage = lambda.grid[i], interaction.depth = 2,
                 cv.folds = 5, n.cores = 2)
    err.grid[i] <- mean(gbm_1$cv.error)
    B.grid[i] <- gbm.perf(gbm_1,plot.it=FALSE,method="cv")
  }

  lamb <- data.frame(lambda=lambda.grid, error=err.grid, B=B.grid)
  lamb.chosen <- lamb$lambda[which.min(lamb$error)]
  B.chosen <- lamb$B[which(lamb$lambda==lamb.chosen)]
  c(lamb.chosen, B.chosen)
}

lambda_1 <- gbm_lambda(1)

p.error.gbm <- function (depth) {
    set.seed(12345)
    train$price_c = as.numeric(as.character(train$price_c))
    test$price_c = as.numeric(as.character(test$price_c))
    gbm_1 <- gbm(price_c~.-price_c, data = train, distribution = "bernoulli",
                 n.trees=lambda_1[2], shrinkage = lambda_1[1], 
                 interaction.depth = depth, n.cores = 2)
    # prediction error
    mypredict.gbm1 <- predict(gbm_1, newdata=test, type="response", n.trees=300)
    p.gbm1.error <- mean(ifelse(mypredict.gbm1 > 0.5, 1, 0) != test$price_c)
    p.gbm1.error
}

p.gbm1.error <- p.error.gbm(1)


## Adaboost
ada_lambda <- function(depth) {
  lambda.grid <- c(0.01,0.001)
  n.lambda <- 1:length(lambda.grid)
  err.grid <- rep(NA, length=length(n.lambda))
  B.grid <- rep(NA, length=length(n.lambda))

  for (i in n.lambda) {
    set.seed(13456)
    train$price_c = as.numeric(as.character(train$price_c))
    test$price_c = as.numeric(as.character(test$price_c))
    ada_1 <- gbm(price_c~.-price_c, data = train, distribution = "adaboost", n.trees=2000, 
                 shrinkage = lambda.grid[i], interaction.depth = 2, 
                 cv.folds = 5, n.cores = 2)
    err.grid[i] <- mean(ada_1$cv.error)
    B.grid[i] <- gbm.perf(ada_1,plot.it=FALSE,method="cv")
  }

  lamb <- data.frame(lambda=lambda.grid, error=err.grid, B=B.grid)
  lamb.chosen <- lamb$lambda[which.min(lamb$error)]
  B.chosen <- lamb$B[which(lamb$lambda==lamb.chosen)]
  c(lamb.chosen, B.chosen)
}

ada_chosen_1 <- ada_lambda(1)

p.error.ada <- function (depth) {
    set.seed(12345)
    train$price_c = as.numeric(as.character(train$price_c))
    test$price_c = as.numeric(as.character(test$price_c))
    ada_1 <- gbm(price_c~.-price_c, data = train, distribution = "adaboost",
                 n.trees=ada_chosen_1[2], shrinkage = ada_chosen_1[1], 
                 interaction.depth = depth,n.cores = 2)
    # prediction error
    mypredict.ada1 <- predict(ada_1, newdata=test, type="response", n.trees=300)
    p.ada1.error <- mean(ifelse(mypredict.ada1 > 0.5, 1, 0) != test$price_c)
    p.ada1.error
}

p.ada1.error <- p.error.ada(1)
```

```{r eval=F}
##### Naive Bayes
NBclassifier <- naive_bayes(price_c~.,usekernel=T, data=train)
pred.nb <- predict(NBclassifier, test, type="class")
pred.error.nb <- mean(test$price_c != pred.nb)
```

```{r eval=F}
##### Tree-based Method
real_p = real[,-which(colnames(real)=="price")]
# train_p = train[,-which(colnames(real)=="price")]
# test_p = test[,-which(colnames(real)=="price")]

mytree = tree(price_c ~.-price_c, data=real_p, method = "gini")
summary(mytree)
plot(mytree);text(mytree,pretty=0,digits=3)

# use CV to choose size of the tree
mytree = tree(price_c ~.-price_c, data=real_p, 
              method = "gini", subset=train_ind)
mytree.cv = cv.tree(mytree, FUN=prune.misclass, K=10)

plot(dev~size, data=as.data.frame(mytree.cv[1:3]),type="b")
points(x=mytree.cv$size[mytree.cv$dev==min(mytree.cv$dev)],
y=rep(min(mytree.cv$dev),sum(mytree.cv$dev==min(mytree.cv$dev))),col="red",pch=19) # dev corresponds to the cross-validation error rate in this instance
# The tree with 4 terminal nodes results in the lowest cross-validation error rate, with 30 cross-validation errors.

final.tree = prune.tree(mytree,
                        best=mytree.cv$size[mytree.cv$dev==min(mytree.cv$dev)])
plot(final.tree); text(final.tree,pretty=3,digits=3)

# assess prediction error
mypredict=predict(final.tree,newdata=
                    real_p[-train_ind,-match("price_c",colnames(real_p))],
                  type="class")
tmp = table(mypredict, real_p$price_c[-train_ind])
1-sum(diag(tmp)/sum(tmp)) # trash 0.1256039
 
# # Boosting
# #Run gbm
# require(gbm)
# real$price_c = ifelse(real$price_c=="low",0,1)
# 
# # to get optimal number of trees
# gbm.real = gbm(price_c~.-price-price_c,data=real,distribution = "bernoulli",n.trees=5000,interaction.depth = 1,
#               shrinkage=0.01,train.fraction = 0.5)
# ## Number of trees via OOB
# gbm.perf(gbm.real,method="OOB") #320
# gbm.perf(gbm.hd,method="test") #1960
# 
# # fit model
# gbm.real = gbm(price_c~.-price-price_c,data=real,distribution = "bernoulli",n.trees=1960,interaction.depth = 1,
#               shrinkage=0.01)
# summary(gbm.real)
```










